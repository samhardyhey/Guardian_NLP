{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'articleCount': 273, 'totalChar': 1732263, 'totalWord': 294321, 'totalTokens': 251, 'tokenSum': 50827, 'topTenTerms': [{'term': 'woman', 'tf': 1037, 'df': 76}, {'term': 'australia', 'tf': 1013, 'df': 192}, {'term': 'one', 'tf': 933, 'df': 207}, {'term': 'year', 'tf': 771, 'df': 208}, {'term': 'government', 'tf': 753, 'df': 151}, {'term': 'people', 'tf': 684, 'df': 176}, {'term': 'make', 'tf': 679, 'df': 200}, {'term': 'get', 'tf': 643, 'df': 149}, {'term': 'work', 'tf': 592, 'df': 154}, {'term': 'go', 'tf': 580, 'df': 158}], 'kMeanClusters': {0: ['child', 'report', 'government', 'go', 'ask'], 1: ['woman', 'work', 'year', 'make', 'one'], 2: ['report', 'system', 'year', 'claim', 'people'], 3: ['one', 'year', 'go', 'get', 'people'], 4: ['government', 'turnbull', 'minister', 'rate', 'service'], 5: ['party', 'nation', 'labor', 'one', 'election'], 6: ['government', 'market', 'policy', 'australia', 'company']}}\n"
     ]
    }
   ],
   "source": [
    "#NLP analysis\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#local modules\n",
    "from retrieve import *\n",
    "from clean import *\n",
    "\n",
    "# initialize constants, lematizer, punctuation and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "#define stopwords\n",
    "custom_stop_words = ['–', '\\u2019', 'u', '\\u201d', '\\u201d.',\n",
    "                     '\\u201c', 'say', 'saying', 'sayings',\n",
    "                     'says', 'us', 'un', '.\\\"', 'would',\n",
    "                     'let', '.”', 'said', ',”'\n",
    "                     ]\n",
    "stopwords = set(sw.words('english') + custom_stop_words)\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    # collapse word inflections into single representation\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # tokenize the corpus\n",
    "    tokens = []\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "def retrieveTopTDIDF(df, main):\n",
    "    # index each term's Term Frequency and Inverse Document Frequency\n",
    "    df = df['bodyText'] # text entries only\n",
    "    \n",
    "    # use count vectorizer to find TF and DF of each term\n",
    "    count_vec = CountVectorizer(tokenizer=cab_tokenizer,\n",
    "                                ngram_range=(1, 2), min_df=0.2, max_df=0.8)\n",
    "    X_count = count_vec.fit_transform(df)\n",
    "\n",
    "    # return total number of tokenized words\n",
    "    totalTokens = len(count_vec.get_feature_names())\n",
    "\n",
    "    # cast numpy integers back to python integers\n",
    "    terms = [{'term': t,\n",
    "              'tf': X_count[:, count_vec.vocabulary_[t]].sum(),\n",
    "              'df': X_count[:, count_vec.vocabulary_[t]].count_nonzero()}\n",
    "             for t in count_vec.vocabulary_]\n",
    "\n",
    "    topTenTerms = sorted(terms, key=lambda k: (\n",
    "        k['tf'], k['df']), reverse=True)[:10]\n",
    "\n",
    "    tokenSum = sum(term['tf'] for term in terms)\n",
    "    \n",
    "    #update main data object\n",
    "    main.update({'totalTokens':totalTokens,'tokenSum':tokenSum,'topTenTerms':topTenTerms})\n",
    "    \n",
    "    return main\n",
    "\n",
    "def createKMeans(df, main):\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=0.2, max_df=0.8)\n",
    "    X = tfidf_vec.fit_transform(df['bodyText'])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=7, random_state=42).fit(X)\n",
    "    \n",
    "    #update main data object\n",
    "    main['kMeanClusters'] = visKMeans(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())\n",
    "    \n",
    "    return main\n",
    "\n",
    "def visKMeans(n_clusters, cluster_centers, terms, num_word = 5):\n",
    "    # find features/terms closest to centroids\n",
    "    ordered_centroids = cluster_centers.argsort()[:, ::-1]\n",
    "    \n",
    "    clusters = dict()\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        temp = []\n",
    "        for term_idx in ordered_centroids[cluster, :5]:\n",
    "            temp.append(terms[term_idx])\n",
    "        clusters[cluster]= temp\n",
    "    \n",
    "    return clusters #formatted clusters\n",
    "    \n",
    "def descriptive(df, main):\n",
    "    #total articles\n",
    "    main['articleCount'] = len(df.index)\n",
    "    \n",
    "    #char tally\n",
    "    main['totalChar'] = df['charCount'].sum()\n",
    "    \n",
    "    #word tally\n",
    "    main['totalWord'] = df['wordcount'].sum()\n",
    "    \n",
    "    return main\n",
    "\n",
    "#descriptive -> macros, tfidf\n",
    "#specific -> LSA, K-means\n",
    "\n",
    "def main():\n",
    "    #retrieve and clean\n",
    "    retrieve_articles()\n",
    "    df = read_in()\n",
    "    df = scrub(df)\n",
    "        \n",
    "    #analyse\n",
    "    main = dict()\n",
    "    main = descriptive(df, main)\n",
    "    main = retrieveTopTDIDF(df, main)\n",
    "    main = createKMeans(df, main)\n",
    "    \n",
    "    print(main)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
