{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from retrieve import *\n",
    "from clean import *\n",
    "\n",
    "retrieve_articles()\n",
    "df = read_in()\n",
    "df = scrub(df)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLP analysis\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#local modules\n",
    "from retrieve import *\n",
    "from clean import *\n",
    "\n",
    "# initialize constants, lematizer, punctuation and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "#define stopwords\n",
    "stopwords = set(sw.words('english'))\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    # collapse word inflections into single representation\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # tokenize the corpus\n",
    "    tokens = []\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "def retrieveTopTDIDF(df):\n",
    "    # index each term's Term Frequency and Inverse Document Frequency\n",
    "    df = df['bodyText'] # text entries only\n",
    "    \n",
    "    print(df.head(5))\n",
    "    print(df.shape)\n",
    "\n",
    "    # use count vectorizer to find TF and DF of each term\n",
    "    count_vec = CountVectorizer(tokenizer=cab_tokenizer,\n",
    "                                ngram_range=(1, 2), min_df=0.2, max_df=0.8)\n",
    "    X_count = count_vec.fit_transform(df)\n",
    "\n",
    "    # return total number of tokenized words\n",
    "    totalTokens = len(count_vec.get_feature_names())\n",
    "\n",
    "    # cast numpy integers back to python integers\n",
    "    terms = [{'term': t,\n",
    "              'tf': X_count[:, count_vec.vocabulary_[t]].sum(),\n",
    "              'df': X_count[:, count_vec.vocabulary_[t]].count_nonzero()}\n",
    "             for t in count_vec.vocabulary_]\n",
    "\n",
    "    topTenTerms = sorted(terms, key=lambda k: (\n",
    "        k['tf'], k['df']), reverse=True)[:10]\n",
    "\n",
    "    tokenSum = sum(term['tf'] for term in terms)\n",
    "\n",
    "    # return top ten terms as well as sum of all tokenizations\n",
    "    return topTenTerms, tokenSum, totalTokens\n",
    "\n",
    "def truncateSVD(df):\n",
    "    # apply singular value decomposition (matrix factorization), retrieve\n",
    "    # prominent clusters\n",
    "    df = df.filter(['bodyText']) # text entries only\n",
    "\n",
    "    # collapse into bag of words representation, limit extreme terms\n",
    "    vector = TfidfVectorizer(tokenizer=cab_tokenizer,\n",
    "                             ngram_range=(1, 2), min_df=0.1, max_df=0.9)\n",
    "    matrix = vector.fit_transform(df)\n",
    "\n",
    "    # generate truncated SVD usingp reivously generated matrix\n",
    "    svd = TruncatedSVD(n_components=20, algorithm='randomized',\n",
    "                       n_iter=5, random_state=42)\n",
    "    svdTrans = svd.fit_transform(matrix)\n",
    "\n",
    "    # sort by term weighting\n",
    "    sorted_comp = svd.components_.argsort()[:, ::-1]\n",
    "    terms = vector.get_feature_names()\n",
    "\n",
    "    # fill with 10 SVD cluster entries\n",
    "    clusterTerms = []\n",
    "\n",
    "    for comp_num in range(10):\n",
    "        clusterTerms.append([terms[i] for i in sorted_comp[comp_num, :5]])\n",
    "\n",
    "    return clusterTerms\n",
    "\n",
    "def main():\n",
    "    #retrieve and clean\n",
    "    retrieve_articles()\n",
    "    df = read_in()\n",
    "    df = scrub(df)\n",
    "\n",
    "    # retrieve most frequent, weighted terms\n",
    "    topTDIDFTerms, tokenSum, totalTokens = retrieveTopTDIDF(df)\n",
    "    \n",
    "    print(topTDIDFTerms, tokenSum, totalTokens)\n",
    "\n",
    "    # retrieve clusters\n",
    "    clusterTerms = truncateSVD(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
