{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2017-03-01\n",
      "...page 1\n",
      "...page 2\n",
      "...page 3\n",
      "...page 4\n",
      "...page 5\n",
      "Writing to articles\\2017-03-01.json\n",
      "Downloading 2017-03-02\n",
      "...page 1\n",
      "...page 2\n",
      "...page 3\n",
      "...page 4\n",
      "...page 5\n",
      "Writing to articles\\2017-03-02.json\n",
      "      type       sectionId    webPublicationDate  \\\n",
      "0  article            news  2017-03-01T23:31:24Z   \n",
      "1  article         society  2017-03-01T23:30:01Z   \n",
      "2  article         us-news  2017-03-01T23:23:19Z   \n",
      "3  article  australia-news  2017-03-01T22:56:11Z   \n",
      "4  article           sport  2017-03-01T22:42:11Z   \n",
      "\n",
      "                                            webTitle  \\\n",
      "0  The 18C debate: how frequent are racial discri...   \n",
      "1  Don't smoke it with tobacco: scientists sugges...   \n",
      "2  Dreamer detained by Ice agents while in proces...   \n",
      "3  Fast-food chains barred from using foreign wor...   \n",
      "4  Transgender-friendly toilets planned for 2020 ...   \n",
      "\n",
      "                                           trailText  \\\n",
      "0  Coalition MPs have repeatedly expressed concer...   \n",
      "1  As more countries relax their laws and with dr...   \n",
      "2  Immigration officials arrested Daniela Vargas,...   \n",
      "3  Peter Dutton announces end to labour agreement...   \n",
      "4  Tokyo’s city government is planning on install...   \n",
      "\n",
      "                         byline wordcount  firstPublicationDate  \\\n",
      "0                 Nick Evershed       365  2017-03-01T23:31:24Z   \n",
      "1     Ian Sample Science editor       722  2017-03-01T23:30:01Z   \n",
      "2  Oliver Laughland in New York       598  2017-03-01T23:23:19Z   \n",
      "3                   Ben Doherty       517  2017-03-01T22:56:11Z   \n",
      "4            Bryan Armen Graham       215  2017-03-01T22:42:11Z   \n",
      "\n",
      "                                            bodyText charCount  \n",
      "0  A parliamentary committee this week handed dow...      2314  \n",
      "1  Scientists are calling for a major effort to m...      4281  \n",
      "2  A young woman applying for the renewal of her ...      3621  \n",
      "3  Fast-food outlets such as McDonald’s, KFC and ...      3177  \n",
      "4  Tokyo’s metropolitan government is seeking to ...      1387  \n"
     ]
    }
   ],
   "source": [
    "from retrieve import *\n",
    "from clean import *\n",
    "\n",
    "retrieve_articles()\n",
    "df = read_in()\n",
    "df = scrub(df)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    A parliamentary committee this week handed dow...\n",
      "1    Scientists are calling for a major effort to m...\n",
      "2    A young woman applying for the renewal of her ...\n",
      "3    Fast-food outlets such as McDonald’s, KFC and ...\n",
      "4    Tokyo’s metropolitan government is seeking to ...\n",
      "Name: bodyText, dtype: object\n",
      "(200,)\n",
      "[{'term': 'say', 'tf': 1116, 'df': 146}, {'term': '–', 'tf': 634, 'df': 147}, {'term': '.”', 'tf': 422, 'df': 123}, {'term': 'year', 'tf': 410, 'df': 145}, {'term': 'one', 'tf': 387, 'df': 129}, {'term': 'would', 'tf': 382, 'df': 114}, {'term': 'make', 'tf': 371, 'df': 121}, {'term': '”', 'tf': 320, 'df': 115}, {'term': ',”', 'tf': 313, 'df': 101}, {'term': 'time', 'tf': 286, 'df': 119}] 22332 172\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dce28c794bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-dce28c794bb0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[1;31m# retrieve clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mclusterTerms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtruncateSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dce28c794bb0>\u001b[0m in \u001b[0;36mtruncateSVD\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    100\u001b[0m     vector = TfidfVectorizer(tokenizer=cab_tokenizer,\n\u001b[1;32m    101\u001b[0m                              ngram_range=(1, 2), min_df=0.2, max_df=0.8)\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[1;31m# generate truncated SVD usingp reivously generated matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    888\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[1;32m    772\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[1;32m    773\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "#NLP analysis\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#local modules\n",
    "from retrieve import *\n",
    "from clean import *\n",
    "\n",
    "# initialize constants, lematizer, punctuation and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "#define stopwords\n",
    "stopwords = set(sw.words('english'))\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    # collapse word inflections into single representation\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # tokenize the corpus\n",
    "    tokens = []\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "def retrieveTopTDIDF(df):\n",
    "    # index each term's Term Frequency and Inverse Document Frequency\n",
    "    df = df['bodyText'] # text entries only\n",
    "    \n",
    "    print(df.head(5))\n",
    "    print(df.shape)\n",
    "\n",
    "    # use count vectorizer to find TF and DF of each term\n",
    "    count_vec = CountVectorizer(tokenizer=cab_tokenizer,\n",
    "                                ngram_range=(1, 2), min_df=0.2, max_df=0.8)\n",
    "    X_count = count_vec.fit_transform(df)\n",
    "\n",
    "    # return total number of tokenized words\n",
    "    totalTokens = len(count_vec.get_feature_names())\n",
    "\n",
    "    # cast numpy integers back to python integers\n",
    "    terms = [{'term': t,\n",
    "              'tf': X_count[:, count_vec.vocabulary_[t]].sum(),\n",
    "              'df': X_count[:, count_vec.vocabulary_[t]].count_nonzero()}\n",
    "             for t in count_vec.vocabulary_]\n",
    "\n",
    "    topTenTerms = sorted(terms, key=lambda k: (\n",
    "        k['tf'], k['df']), reverse=True)[:10]\n",
    "\n",
    "    tokenSum = sum(term['tf'] for term in terms)\n",
    "\n",
    "    # return top ten terms as well as sum of all tokenizations\n",
    "    return topTenTerms, tokenSum, totalTokens\n",
    "\n",
    "def truncateSVD(df):\n",
    "    # apply singular value decomposition (matrix factorization), retrieve\n",
    "    # prominent clusters\n",
    "    df = df.filter(['bodyText']) # text entries only\n",
    "\n",
    "    # collapse into bag of words representation, limit extreme terms\n",
    "    vector = TfidfVectorizer(tokenizer=cab_tokenizer,\n",
    "                             ngram_range=(1, 2), min_df=0.1, max_df=0.9)\n",
    "    matrix = vector.fit_transform(df)\n",
    "\n",
    "    # generate truncated SVD usingp reivously generated matrix\n",
    "    svd = TruncatedSVD(n_components=20, algorithm='randomized',\n",
    "                       n_iter=5, random_state=42)\n",
    "    svdTrans = svd.fit_transform(matrix)\n",
    "\n",
    "    # sort by term weighting\n",
    "    sorted_comp = svd.components_.argsort()[:, ::-1]\n",
    "    terms = vector.get_feature_names()\n",
    "\n",
    "    # fill with 10 SVD cluster entries\n",
    "    clusterTerms = []\n",
    "\n",
    "    for comp_num in range(10):\n",
    "        clusterTerms.append([terms[i] for i in sorted_comp[comp_num, :5]])\n",
    "\n",
    "    return clusterTerms\n",
    "\n",
    "def main():\n",
    "    #retrieve and clean\n",
    "    retrieve_articles()\n",
    "    df = read_in()\n",
    "    df = scrub(df)\n",
    "\n",
    "    # retrieve most frequent, weighted terms\n",
    "    topTDIDFTerms, tokenSum, totalTokens = retrieveTopTDIDF(df)\n",
    "    \n",
    "    print(topTDIDFTerms, tokenSum, totalTokens)\n",
    "\n",
    "    # retrieve clusters\n",
    "    clusterTerms = truncateSVD(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
