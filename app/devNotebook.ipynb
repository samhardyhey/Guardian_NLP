{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2017-03-01\n",
      "...page 1\n",
      "Writing to articles/2017-03-01.json\n",
      "Downloading 2017-03-02\n",
      "...page 1\n",
      "Writing to articles/2017-03-02.json\n",
      "CPU times: user 42.6 ms, sys: 35.5 ms, total: 78.1 ms\n",
      "Wall time: 3.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# article retrieval\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from os import mkdir\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# define dev key and query object\n",
    "MY_API_KEY = retrieve_key()\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "my_params = {\n",
    "    'from-date': \"\",\n",
    "    'to-date': \"\",\n",
    "    'order-by': \"newest\",\n",
    "    'show-fields': 'all',\n",
    "    'page-size': 50,\n",
    "    'production-office': 'AUS',\n",
    "    'lang': 'en',\n",
    "    'page': 1,\n",
    "    'api-key': MY_API_KEY\n",
    "}\n",
    "\n",
    "def retrieve_articles(start_date, end_date):\n",
    "    dayrange = range((end_date - start_date).days + 1)\n",
    "    \n",
    "    # store articles\n",
    "    ARTICLES_DIR = 'articles'\n",
    "\n",
    "    for daycount in dayrange:\n",
    "        dt = start_date + timedelta(days=daycount)\n",
    "        datestr = dt.strftime('%Y-%m-%d')\n",
    "        fname = join(ARTICLES_DIR, datestr + '.json')\n",
    "\n",
    "        if not exists(fname):\n",
    "            # then let's download it\n",
    "            print(\"Downloading\", datestr)\n",
    "            all_results = []\n",
    "            my_params['from-date'] = datestr\n",
    "            my_params['to-date'] = datestr\n",
    "            current_page = 1\n",
    "            total_pages = 1\n",
    "            \n",
    "            while current_page <= total_pages:\n",
    "                print(\"...page\", current_page)\n",
    "                my_params['page'] = current_page\n",
    "                resp = requests.get(API_ENDPOINT, my_params)\n",
    "                data = resp.json()\n",
    "                all_results.extend(data['response']['results'])\n",
    "                # if there is more than one page\n",
    "                current_page += 1\n",
    "                total_pages = data['response']['pages']\n",
    "\n",
    "            with open(fname, 'w') as f:\n",
    "                print(\"Writing to\", fname)\n",
    "                # re-serialize it for pretty indentation\n",
    "                f.write(json.dumps(all_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'372adb16-2704-4b36-9f99-3472dd5ac682'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def flatten(article):\n",
    "    # collapse internal feature dictionary\n",
    "    if isinstance(article, dict):\n",
    "        temp = {}\n",
    "        temp.update(article['fields'])\n",
    "        article.pop('fields')\n",
    "        temp.update(article)\n",
    "        return temp\n",
    "\n",
    "def read_in():\n",
    "    # collapse all articles into single df\n",
    "    files = os.listdir('./articles')\n",
    "    articles = []\n",
    "\n",
    "    # flatten and append each article\n",
    "    for each in files:\n",
    "        try: #ensure parsing is encoding-resistant..\n",
    "            with open('./articles/' + each) as json_data:\n",
    "                for article in json.load(json_data):\n",
    "                    articles.append(flatten(article))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    df = pd.DataFrame(articles)\n",
    "    return df\n",
    "\n",
    "\n",
    "def scrub(df):\n",
    "    # include useful components only\n",
    "    df = df.filter(['type', 'sectionId', 'webPublicationDate', 'webTitle',\n",
    "                    'trailText', 'byline', 'wordcount', 'firstPublicationDate',\n",
    "                    'bodyText', 'charCount'])\n",
    "\n",
    "    # cast date/time types\n",
    "    df['webPublicationDate'] = pd.to_datetime(df['webPublicationDate'])\n",
    "    df['firstPublicationDate'] = pd.to_datetime(df['firstPublicationDate'])\n",
    "    \n",
    "    #generate meta information for each article\n",
    "    df['wordcount'] = df['wordcount'].astype(int)\n",
    "    df['charCount'] = df['charCount'].astype(int)\n",
    "    \n",
    "    df.dropna() #all nan and 0 field entries\n",
    "    df = df.loc[(df!=0).any(axis=1)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def export_frame(file_name, df):\n",
    "    #single frame export\n",
    "    df.to_csv(file_name,index=False)\n",
    "\n",
    "def write_json(file_name, results):\n",
    "    # Writing JSON data\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "def retrieve_key():\n",
    "    with open('./keys/guardian_key.txt') as f:\n",
    "        data = f.readlines()\n",
    "    return data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLP analysis\n",
    "import string, time\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "# #local modules\n",
    "# from retrieve import *\n",
    "# from clean import *\n",
    "\n",
    "# initialize constants, lematizer, punctuation and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "#define stopwords\n",
    "custom_stop_words = ['–', '\\u2019', 'u', '\\u201d', '\\u201d.',\n",
    "                     '\\u201c', 'say', 'saying', 'sayings',\n",
    "                     'says', 'us', 'un', '.\\\"', 'would',\n",
    "                     'let', '.”', 'said', ',”'\n",
    "                     ]\n",
    "stopwords = set(sw.words('english') + custom_stop_words)\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    # collapse word inflections into single representation\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # tokenize the corpus\n",
    "    tokens = []\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "def format_topics(model,feature_names,no_top_words,time_elapsed):\n",
    "    #top words for topic within given decomposition model\n",
    "    analysis = dict()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_placeholder = \"Topic {}\".format(topic_idx)\n",
    "        analysis[topic_placeholder] = [feature_names[i] for i in (-topic).argsort()[:no_top_words]]\n",
    "    analysis['time_sec'] = time_elapsed\n",
    "    return analysis\n",
    "\n",
    "def nmf(df,main,topics):\n",
    "    #NMF requires TFIDF vectorizer\n",
    "    st = time.time()\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=cab_tokenizer,ngram_range=(1,2),\n",
    "                                       min_df=0.1, max_df=0.90)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df['bodyText'])\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names() #fit, transform, retrieve features names\n",
    "    \n",
    "    #Non-Negative Matrix Factorization - fit model using tfidf vector\n",
    "    nmf = NMF(n_components=topics,random_state=1,alpha=0.1,l1_ratio=0.5,init='nndsvd').fit(tfidf)\n",
    "    et = time.time() - st\n",
    "    \n",
    "    main['nmf'] = format_topics(nmf, tfidf_feature_names, topics, et)\n",
    "    return main\n",
    "\n",
    "def lda(df,main,topics):\n",
    "    #LDA requires Count Vectorizer\n",
    "    st = time.time()\n",
    "    tf_vectorizer = CountVectorizer(tokenizer=cab_tokenizer,ngram_range=(1,2),\n",
    "                                       min_df=0.1, max_df=0.90)\n",
    "    tf = tf_vectorizer.fit_transform(df['bodyText'])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    #Latent Dirilicht Analysis - fit the model using term frequency vector\n",
    "    lda = LatentDirichletAllocation(n_components=topics,max_iter=5,learning_method='online',learning_offset=50,random_state=0).fit(tf)\n",
    "    et = time.time() - st\n",
    "    \n",
    "    main['lda'] = format_topics(lda, tf_feature_names, topics, et)\n",
    "    return main\n",
    "    \n",
    "def lsa(df,main,topics):\n",
    "    #Singular Value Decomposition becomes Latent Semantic Analysis when paired with tfidf vectorizer\n",
    "    st = time.time()\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=cab_tokenizer,ngram_range=(1,2),\n",
    "                                       min_df=0.1, max_df=0.90)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df['bodyText'])\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names() #fit, transform, retrieve features names\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=topics, algorithm='randomized', n_iter=5, random_state=42).fit(tfidf)\n",
    "    et = time.time() - st\n",
    "    \n",
    "    main['lsa'] = format_topics(svd, tfidf_feature_names, topics, et)\n",
    "    return main\n",
    "    \n",
    "def descriptive(df, main):\n",
    "    #overview of provided df\n",
    "    main['articleCount'] = len(df.index)\n",
    "    main['totalChar'] = df['charCount'].sum()\n",
    "    main['totalWord'] = df['wordcount'].sum()\n",
    "    \n",
    "    return main\n",
    "\n",
    "def all_analysis(df, label):\n",
    "    #all desciptive stats, decomposition methods for a provided df\n",
    "    main_analysis = dict()\n",
    "    main_analysis['name'] = label\n",
    "    main_analysis = descriptive(df,main_analysis)\n",
    "    main_analysis = nmf(df,main_analysis,10)\n",
    "    main_analysis = lda(df,main_analysis,10)\n",
    "    main_analysis = lsa(df,main_analysis,10)\n",
    "    return main_analysis\n",
    "\n",
    "#To Do\n",
    "#segmentation => author, time period, topic tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'all_articles', 'articleCount': 61, 'totalChar': 341483, 'totalWord': 57682, 'nmf': {'Topic 0': ['one', 'go', 'get', 'time', 'work', 'play', 'people', 'year', 'need', 'come'], 'Topic 1': ['labor', 'party', 'suggest', 'nation', 'coalition', 'key', 'majority', 'liberal', 'enough', 'election'], 'Topic 2': ['growth', 'quarter', 'economy', 'price', 'wage', 'gdp', '1', 'figure', 'market', 'rate'], 'Topic 3': ['information', 'release', 'department', 'personal', 'security', 'social', 'detail', 'recipient', 'legal', 'minister'], 'Topic 4': ['18c', 'complaint', 'discrimination', 'section', 'speech', 'committee', 'right', 'act', 'australian', 'liberal'], 'Topic 5': ['government', 'power', 'coal', 'minister', 'project', 'party', 'agreement', 'fund', 'australia', 'policy'], 'Topic 6': ['woman', 'men', 'set', 'executive', 'news', 'figure', '‘', 'appear', 'see', 'pay'], 'Topic 7': ['worker', 'penalty', 'rate', 'cut', 'turnbull', 'pay', 'commission', 'labor', 'minister', 'support'], 'Topic 8': ['trump', 'campaign', 'committee', 'investigation', 'election', 'political', 'official', 'president', 'senator', 'meet'], 'Topic 9': ['average', 'event', 'year', 'experience', 'climate', 'increase', 'sydney', 'record', 'australia', 'chance'], 'time_sec': 4.287829160690308}, 'lda': {'Topic 0': ['government', 'australia', 'australian', 'food', 'minister', 'fund', 'new', 'get', 'year', 'use'], 'Topic 1': ['growth', 'quarter', 'year', 'abc', 'figure', 'complaint', '1', 'wage', 'economy', 'grow'], 'Topic 2': ['information', 'release', 'department', 'personal', 'minister', 'social', 'security', 'law', 'make', 'public'], 'Topic 3': ['one', 'year', 'make', '18c', 'go', 'report', 'get', 'australian', 'people', 'know'], 'Topic 4': ['government', 'rate', 'cut', 'penalty', 'minister', 'labor', 'work', 'turnbull', 'worker', 'year'], 'Topic 5': ['woman', 'one', 'australia', 'work', 'also', 'time', 'new', 'people', 'get', 'year'], 'Topic 6': ['information', 'release', 'could', 'fund', 'fair', 'executive', 'personal', 'result', 'australia', 'system'], 'Topic 7': ['water', 'party', 'campaign', '2', 'labor', 'liberal', '10', 'seek', 'prime', 'predict'], 'Topic 8': ['government', 'labor', 'power', 'agreement', 'minister', 'action', 'decision', 'allow', 'law', 'one'], 'Topic 9': ['child', 'time', 'government', 'people', 'family', 'party', 'care', 'political', 'take', 'labor'], 'time_sec': 4.36387825012207}, 'lsa': {'Topic 0': ['government', 'australia', 'year', 'one', 'work', 'australian', 'people', 'make', 'get', 'time'], 'Topic 1': ['woman', 'play', 'time', 'go', 'get', 'life', 'one', 'like', 'night', 'mark'], 'Topic 2': ['growth', 'quarter', 'economy', 'price', 'rate', '1', 'market', 'year', 'gdp', 'wage'], 'Topic 3': ['information', 'release', 'department', 'personal', 'social', 'woman', 'security', 'growth', 'detail', 'legal'], 'Topic 4': ['18c', 'complaint', 'discrimination', 'section', 'speech', 'right', 'experience', 'act', 'human right', 'australian'], 'Topic 5': ['woman', 'government', 'power', 'agreement', 'fund', 'coal', 'australia', 'action', 'project', 'vulnerable'], 'Topic 6': ['woman', 'trump', 'campaign', 'news', 'men', 'figure', 'labor', 'set', 'political', 'election'], 'Topic 7': ['woman', 'worker', 'food', 'penalty', 'agreement', 'rate', 'cut', 'pay', 'commission', 'work'], 'Topic 8': ['trump', 'investigation', 'agreement', 'campaign', 'federal', 'market', 'claim', 'vulnerable', 'food', 'president'], 'Topic 9': ['child', 'family', 'care', 'young', 'immigration', 'home', 'coal', 'many', 'trump', 'generation'], 'time_sec': 4.194566249847412}}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #1.0 retrieve\n",
    "    retrieve_articles(date(2017, 3, 1), date(2017, 3, 2))\n",
    "    \n",
    "    #2.0 clean\n",
    "    df = read_in()\n",
    "    df = scrub(df)\n",
    "    export_frame('./output/corpus.csv', df)\n",
    "        \n",
    "    #3.0 analyse\n",
    "    analysis = all_analysis(df,'all_articles')\n",
    "    print(analysis)\n",
    "    \n",
    "    #4.0 write results\n",
    "    write_json('./output/analysis.json', analysis)\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
